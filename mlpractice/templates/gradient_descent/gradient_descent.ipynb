{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring gradient descent\n",
    "Step formula:\n",
    "$$\n",
    "\\eta_k = \\lambda(\\frac{s_0}{s_0 + k})^p\n",
    "$$\n",
    "You don't need to set $s_0$ or $p_0$, you can use default $1$ and $0.5$, but you should adjust $\\lambda$\n",
    "\n",
    "In this task we use MSE loss function:\n",
    "$$\n",
    "Q(w) = \\frac{1}{l}\\sum\\limits_{i=1}^l (a_w(x_i) - y_i)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.BaseDescent>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "$$\n",
    "w_{k+1} = w_k - \\eta_k \\nabla_w Q(w_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.GradientDescent>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from mlpractice.tests.gradient_descent.test_gradient_descent import test_all\n",
    "# GradDesc = GradientDescent() # TODO\n",
    "# test_all(GradDesc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Descent\n",
    "\n",
    "$$\n",
    "w_{k+1} = w_k - \\eta_k \\nabla_w q_{i_k}(w_k)\n",
    "$$\n",
    "where $\\nabla_w q_{i_k}(w_k)$ - gradient estimation for batch with randomly selected objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.StochasticDescent>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from mlpractice.tests.gradient_descent.test_stochastic_descent import test_all\n",
    "# StochDesc = StochasticDescent() # TODO\n",
    "# test_all(StochDesc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Descent\n",
    "\n",
    "$$\n",
    "h_0 = 0, h_{k+1} = \\alpha h_k + \\eta_k \\nabla_w Q(w_k),\\\\\n",
    "w_{k+1} = w_k - h_{k + 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.MomentumDescent>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from mlpractice.tests.gradient_descent.test_momentum_descent import test_all\n",
    "# MomhDesc = MomentumDescent() # TODO\n",
    "# test_all(MomDesc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "\n",
    "$$\n",
    "G_0 = 0, G_{k+1} = G_k + (\\nabla_w Q(w_k))^2,\\\\\n",
    "w_{k+1} = w_k - \\frac{\\eta_k}{\\sqrt{\\varepsilon + G_{k+1}}} \\nabla_k Q(w_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.Adagrad>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from mlpractice.tests.gradient_descent.test_adagrad import test_all\n",
    "# Adagr = Adagrad() # TODO\n",
    "# test_all(Adagr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent in action\n",
    "## Linear Regression\n",
    "To see how gradient descent can provide minimizing loss, we propose the implementation of linear regression, that studying with using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.LinearRegression>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from mlpractice.tests.gradient_descent.test_linear_regression import test_all\n",
    "# regression = LinearRegression() # TODO\n",
    "# test_all(regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "$$\n",
    "G(w) = \\frac{1}{l}\\sum\\limits_{i=1}^l (a_w(x_i) - y_i)^2 + \\frac{\\mu}{2}||w||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.GradientDescentReg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from mlpractice.tests.gradient_descent.test_gradient_reg import test_all\n",
    "# GradReg = GradientDescentReg() # TODO\n",
    "# test_all(GradReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.StochasticDescentReg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from mlpractice.tests.gradient_descent.test_stochastic_reg import test_all\n",
    "# StochReg = StochasticDescentReg() # TODO\n",
    "# test_all(StochReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.MomentumDescentReg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from mlpractice.tests.gradient_descent.test_momentum_reg import test_all\n",
    "# MomReg = MomentumDescentReg() # TODO\n",
    "# test_all(MomReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.AdagradReg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from mlpractice.tests.gradient_descent.test_adagrad_reg import test_all\n",
    "# AdagradReg = AdagradDescentReg() # TODO\n",
    "# test_all(AdagradReg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
