{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# Exploring gradient descent with linear regression\n",
    "Task is to build a function as close as possible to the desired using set (argument; value) built on it.\n",
    "And we now what kind of function we expect.\n",
    "\n",
    "For example, based on the situation we conclude that the function should look\n",
    "like $c_1 + c_2 x + c_3 sin x$, but we have only $l$ points and don't know values of constants - this is linear regression that\n",
    "is trained using gradient descent job.\n",
    "\n",
    "So there are $l$ points - $(x_1, y_1), ..., (x_l, y_l)$. We take vector $w = (w_1, w_2, w_3)^T$ and let's call $\\color{cyan}{X}$ this matrix:\n",
    "$\\left(\n",
    "    \\begin{array}{ccc}\n",
    "        1 & x_1 & sin x_1 \\\\\n",
    "        1 & x_2 & sin x_2 \\\\\n",
    "        \\vdots & \\ddots & \\vdots \\\\\n",
    "        1 & x_l & sin x_l\n",
    "    \\end{array}\n",
    "\\right)$\n",
    "and Y this vector: $\\left(\\begin{array}{cccc} y_1 & y_2 & ... & y_l \\end{array}\\right)^T$.\n",
    "\n",
    "$a_w(x) = w_1 + w_2x + w_3 sin x$ - modulated function. Notice that $\\left(\\begin{array}{cccc} a_w(x_1) & a_w(x_2) & ... & a_w(x_l) \\end{array}\\right)^T$ - \n",
    "model (linear regression) response vector is just ${\\color{cyan}{X}} \\cdot w$.\n",
    "\n",
    "And now we come to questions: what is gradient descent and how does it works? Gradient descent is a way to find the extremum of a function\n",
    "by ''scating'' throw function uphill and into fault. In our situation function is loss/difference between modulated and desired and we want to minimize it. \n",
    "\n",
    "As is known from the course of mathematical analysis, the direction of movement is the gradient of the function. Gradient descent takes this value and\n",
    "recalculates the model data according to the formulas, which will be given next.\n",
    "\n",
    "Let's move on the assignments:\n",
    "\n",
    "Step formula:\n",
    "$$\n",
    "\\eta_k = \\lambda(\\frac{s_0}{s_0 + k})^p\n",
    "$$\n",
    "You don't need to set $s_0$ or $p_0$, you can use default $1$ and $0.5$, but you should adjust $\\lambda$\n",
    "\n",
    "In this task we use MSE loss function:\n",
    "$$\n",
    "Q(w) = \\frac{1}{l}\\sum\\limits_{i=1}^l (a_w(x_i) - y_i)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.BaseValues>\n",
    "\n",
    "#!source<mlpractice.gradient_descent.BaseDescent>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "$$\n",
    "w_{k+1} = w_k - \\eta_k \\nabla_w Q(w_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.GradientDescent>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlpractice.tests.gradient_descent.test_gradient_descent import test_all\n",
    "\n",
    "test_all(GradientDescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stochastic Descent\n",
    "\n",
    "$$\n",
    "w_{k+1} = w_k - \\eta_k \\nabla_w q_{i_k}(w_k)\n",
    "$$\n",
    "where $\\nabla_w q_{i_k}(w_k)$ - gradient estimation for batch with randomly selected objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.StochasticDescent>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlpractice.tests.gradient_descent.test_stochastic_descent import test_all\n",
    "\n",
    "test_all(StochasticDescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Momentum Descent\n",
    "\n",
    "$$\n",
    "h_0 = 0, h_{k+1} = \\alpha h_k + \\eta_k \\nabla_w Q(w_k),\\\\\n",
    "w_{k+1} = w_k - h_{k + 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.MomentumDescent>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlpractice.tests.gradient_descent.test_momentum_descent import test_all\n",
    "\n",
    "test_all(MomentumDescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adagrad\n",
    "\n",
    "$$\n",
    "G_0 = 0, G_{k+1} = G_k + (\\nabla_w Q(w_k))^2,\\\\\n",
    "w_{k+1} = w_k - \\frac{\\eta_k}{\\sqrt{\\varepsilon + G_{k+1}}} \\nabla_k Q(w_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.Adagrad>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlpractice.tests.gradient_descent.test_adagrad import test_all\n",
    "\n",
    "test_all(Adagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Gradient Descent in action\n",
    "## Linear Regression\n",
    "To see how gradient descent can provide minimizing loss, we propose the implementation of linear regression, that studying with using gradient descent.\n",
    "\n",
    "Notice that you must comply with following conditions:\n",
    "- Ð¡alculations must be vectorized\n",
    "- Python cycles are only allowed for gradient descent iterations\n",
    "- Stop studying is reaching maximal iteration count (max_iter) or reaching small error (square of the euclidean norm of difference\n",
    "in weights between adjacent iterations is less than tolerance)\n",
    "- Saving loss function history in loss_history from zero step (before studying) to last\n",
    "- Weights must be initialized either to zero or from normal distribution $N(0, 1)$ with fixed seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.LinearRegression>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlpractice.tests.gradient_descent.test_linear_regression import test_all\n",
    "\n",
    "test_all(LinearRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Regularization\n",
    "In this task we want to explore how regularization (adding a fine proportional to the norm of weights). Use the l2-regularization:\n",
    "$$\n",
    "G(w) = \\frac{1}{l}\\sum\\limits_{i=1}^l (a_w(x_i) - y_i)^2 + \\frac{\\mu}{2}||w||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.GradientDescentReg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlpractice.tests.gradient_descent.test_gradient_descent_reg import test_all\n",
    "\n",
    "test_all(GradientDescentReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.StochasticDescentReg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlpractice.tests.gradient_descent.test_stochastic_descent_reg import test_all\n",
    "\n",
    "test_all(StochasticDescentReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.MomentumDescentReg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlpractice.tests.gradient_descent.test_momentum_descent_reg import test_all\n",
    "\n",
    "test_all(MomentumDescentReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.gradient_descent.AdagradReg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mlpractice.tests.gradient_descent.test_adagrad_reg import test_all\n",
    "\n",
    "test_all(AdagradReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If you want to submit you solutions authorize\n",
    "USERNAME = \"\"\n",
    "PASSWORD = \"\"\n",
    "\n",
    "from mlpractice.stats.stats_utils import _get_stats, submit\n",
    "\n",
    "submit(USERNAME, PASSWORD, str(_get_stats()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}