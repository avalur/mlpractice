{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Linear multiclass classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "\n",
    "Softmax is a generalization of logistic output function $\\sigma(M) = \\frac{1}{1+e^M}$ for multiclass classification problem. This softmax function $\\xi$ takes as an input $N$-dimensional vector $z$ and outputs $N$-dimensional vector $y$ of real values between 0 and 1:\n",
    "\n",
    "$$\n",
    "y_n = \\xi(z)_n = \\frac{e^{z_n}}{\\sum_{i=1}^N e^{z_n}} \\ \\ \\text{for} \\ n = 1, \\ldots, N\n",
    "$$\n",
    "\n",
    "We can write the probabilities that the class is $c = n$ for $n = 1, \\ldots, N$ given input $z$ as:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "P(c = 1 | z)\\\\\n",
    "\\vdots\\\\\n",
    "P(c = N | z)\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\xi(z)_1\\\\\n",
    "\\vdots\\\\\n",
    "\\xi(z)_N\n",
    "\\end{pmatrix}\n",
    "=\\frac{1}{\\sum_{i=1}^N e^{z_n}}\n",
    "\\begin{pmatrix}\n",
    "e^{z_1}\\\\\n",
    "\\vdots\\\\\n",
    "e^{z_N}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Where $P(c = n | z)$ is thus the probability that the class is $n$ given the input $z$. Now implement softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#!source<mlpractice.linear_classifier.softmax>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpractice.tests.linear_classifier.test_softmax import test_all\n",
    "\n",
    "test_all(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy loss\n",
    "\n",
    "Cross-entropy loss function $\\zeta(t, z)$:\n",
    "$$\n",
    "\\zeta(t, z) = -\\sum\\limits_{n=1}^N t_n\\log P(c = n|z) = -\\log P(c = c^{true}),\n",
    "$$\n",
    "where $t_n = 1$ if and only if $z$ belongs to class n. So cross-entropy function over a batch of multiple samples size $k$ can be caltulated as:\n",
    "$$\n",
    "\\zeta(T, Z) = \\sum\\limits_{i=1}^k \\zeta(t_i, z_i) = -\\sum\\limits_{i=1}^k \\sum\\limits_{n=1}^N t_{i,n}\\log P(c = n | z_i) = -\\sum\\limits_{i=1}^k \\log P(c = c_i^{true}|z_i)\n",
    "$$\n",
    "where $t_{i,n} = 1$ if and only if $z_i$ belongs to class $n$. And we also see that minimizing this function we maximize likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!source<mlpractice.linear_classifier.cross_entropy_loss>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpractice.tests.linear_classifier.test_cross_entropy_loss import test_all\n",
    "\n",
    "test_all(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax_with_cross_entropy description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!source<mlpractice.linear_classifier.softmax_with_cross_entropy>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpractice.tests.linear_classifier.test_softmax_with_cross_entropy import test_all\n",
    "\n",
    "test_all(softmax_with_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l2_regularization description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!source<mlpractice.linear_classifier.l2_regularization>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpractice.tests.linear_classifier.test_l2_regularization import test_all\n",
    "\n",
    "test_all(l2_regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear_softmax description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!source<mlpractice.linear_classifier.linear_softmax>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpractice.tests.linear_classifier.test_linear_softmax import test_all\n",
    "\n",
    "test_all(linear_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSoftmaxClassifier description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!source<mlpractice.linear_classifier.LinearSoftmaxClassifier>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some testing code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
